import base64
import tempfile
import io
import os

import numpy as np
import torch
import torchaudio

import auditok
from transformers import pipeline
from whisper_timestamped.transcribe import remove_non_speech, get_vad_segments, do_convert_timestamps

RESAMPLE_RATE = 16000
error_dir='/home/ubuntu/s3bucket/errors/'
model_id = "openai/whisper-large-v3-turbo"
generate_kwargs = {
    # "max_new_tokens": 448,
    'max_new_tokens':192,
    "num_beams": 1,
    # "condition_on_prev_tokens": False,
    # "compression_ratio_threshold": 1.35,  # zlib compression ratio threshold (in token space)
    "temperature": (0.0, 0.2, 0.4, 0.6, 0.8, 1.0),
    # "logprob_threshold": -1.0,
    # "no_speech_threshold": 0.6,
    "return_timestamps": True,
    "language": "english"
}

def model_fn(model_dir):
    """
    Load the model from the specified directory.
    """
    gpu_on = torch.cuda.is_available()

    model = pipeline(
        "automatic-speech-recognition",
        model=model_dir,
        # max_new_tokens=192,
        # device="cuda:0" if gpu_on else "cpu",
        torch_dtype=torch.float16 if gpu_on else torch.float32
    )
    print('device',model.device)
    return model
def decode_input_from_file(audio_file):
 # convert .wav to audio tensor
    tensor, orig_sr = torchaudio.load(audio_file)
    tensor = torch.mean(tensor, dim=0, keepdim=False) # mono

    # near-lossless resampling
    return torchaudio.functional.resample(
        tensor,
        orig_sr, 
        new_freq=RESAMPLE_RATE, 
        lowpass_filter_width=64,
        rolloff=0.9475937167399596,
        resampling_method="sinc_interp_kaiser",
        beta=14.769656459379492
    )

def decode_input(input_b64):
    """
    Decode the input data from b64 string to audio tensor sampled at 16kHz
    """
    # decode the input data (b64 string to tempfile)
    b64 = input_b64.encode("utf-8")
    wav = base64.decodebytes(b64)

    # write audio bytes to .wav
    file = io.BytesIO(wav)
    tfile = tempfile.NamedTemporaryFile(delete=False)
    tfile.write(file.read())

    # convert .wav to audio tensor
    tensor, orig_sr = torchaudio.load(tfile.name)
    tensor = torch.mean(tensor, dim=0, keepdim=False) # mono

    # near-lossless resampling
    return torchaudio.functional.resample(
        tensor,
        orig_sr, 
        new_freq=RESAMPLE_RATE, 
        lowpass_filter_width=64,
        rolloff=0.9475937167399596,
        resampling_method="sinc_interp_kaiser",
        beta=14.769656459379492
    )


def make_timestamps_monotonic(transcript):
    """
    Corrects timestamps from the "sequential" Whisper pipeline, which are reset every 30 seconds.
    """
    adjusted_transcript = []
    offset = 0.0
    for i, chunk in enumerate(transcript):
        start_time = chunk["timestamp"][0]
        end_time = chunk["timestamp"][1]
        text = chunk["text"]
        # If the start_time is less than the end_time of the previous tuple, it means a reset has occurred
        if i > 0 and start_time < transcript[i - 1]["timestamp"][1]:
            # Update the offset to account for the reset
            offset += transcript[i - 1]["timestamp"][1] + start_time
        
        # Adjust the start_time and end_time by adding the offset
        adjusted_start_time = round(start_time + offset, 2)
        adjusted_end_time = round(end_time + offset, 2)
        
        # Append the adjusted tuple to the list
        adjusted_transcript.append({
            "timestamp": (adjusted_start_time, adjusted_end_time),
            "text": text
        })
    
    return adjusted_transcript


def undo_silence_removal(voice_segments, transcript):
    """
    Maps timestamps generated by the model on audio where silence has been removed
    back to the timeline of the original call with silent periods.
    """
    # Calculate the cumulative duration of silences up to each point in time
    cumulative_silence = []
    total_silence = 0.0

    # Prepend voice_segments with dummy timestamp (0.0, 0.0)
    voice_segments.insert(0, (0.0, 0.0))
    for i in range(len(voice_segments) - 1):
        end_time = voice_segments[i][1]
        start_time_next = voice_segments[i + 1][0]
        silence_duration = start_time_next - end_time
        total_silence += silence_duration
        cumulative_silence.append((end_time, total_silence))
    
    # Function to get the cumulative silence duration up to a given time
    def get_cumulative_silence(time):
        for end_time, silence in cumulative_silence:
            if time < end_time:
                return silence
        return total_silence

    # Adjust the sentence timestamps
    adjusted_transcript = []
    for chunk in transcript:
        start_time = chunk["timestamp"][0]
        end_time = chunk["timestamp"][1]
        text = chunk["text"]
        adjusted_start_time = round(start_time + get_cumulative_silence(start_time), 2)
        adjusted_end_time = round(end_time + get_cumulative_silence(end_time), 2)
        adjusted_transcript.append({
            "timestamp": (adjusted_start_time, adjusted_end_time),
            "text": text
        })
    
    return adjusted_transcript


def predict_fn(audio_file, model):
    """
    Generate transcript for the incoming request using the model.
    """
    # tensor = decode_input(data)
    tensor = decode_input_from_file(audio_file)

    # remove speech gaps from audio
    audio, voice_segments, _ = remove_non_speech(
        tensor,
        method="auditok",
        sample_rate=RESAMPLE_RATE,
        avoid_empty_speech=True
    )
    # result = model([audio_file ], return_timestamps=True,batch_size=1)
    pred = model(audio.numpy(),return_timestamps=True, generate_kwargs=generate_kwargs)

    # adjust timestamps to correct for chunked timestamps and removed silence
    pred["chunks"] = make_timestamps_monotonic(pred["chunks"])
    pred["chunks"] = undo_silence_removal(voice_segments, pred["chunks"])

    # return dictionary, which will be json serializible
    return pred
import ray
@ray.remote(num_gpus=0.5)
def remoteJob(instance_id,file_list):
    runtime_context = ray.get_runtime_context()
    node_ip =  ray._private.services.get_node_ip_address()
    env_vars = os.environ

    # 打印所有环境变量
    for key, value in env_vars.items():
        print(f"{key}={value}")
    print({
        "node_id": runtime_context.get_node_id(),
        "node_ip":node_ip,
        "task_id": runtime_context.get_task_id(),
    })
   
    
    model =model_fn(model_id)
    for file in file_list:
        try:
            prediction = predict_fn(file, model)
        except Exception as e:
            print(file, ' has  errors ',e)
            if not os.path.exists(error_dir):
                    os.makedirs(error_dir)
            with open(error_dir+  os.path.basename(file), "w") as file:
                gpu_id = os.environ.get("CUDA_VISIBLE_DEVICES", "CPU")
                file.write(f"file {file},node {node_ip}, gpu {gpu_id}, error: {e}")
    # Print the prediction
    # print('ray node ',prediction)
    return file_list
    
    
from concurrent.futures import ThreadPoolExecutor, as_completed
def distribute_files_to_instances(files, num_instances):
    """
    将文件分配到不同的实例上，尽量平均分担
    """
    instance_assignments = [[] for _ in range(num_instances)]
    for idx, file_path in enumerate(files):
        instance_assignments[idx % num_instances].append(file_path)
    return instance_assignments
def process_main(instance_id,file_list):
    print(instance_id,'processing ', file_list)
    
    remote = remoteJob.remote(instance_id,file_list)
    ray.get(remote)
    return file_list
def process_files_with_instances(files, num_instances):
    """
    使用多线程处理文件，分配到多个实例上
    """
    # 平均分配文件到实例
    instance_assignments = distribute_files_to_instances(files, num_instances)

    # 使用多线程触发每个实例处理文件
    with ThreadPoolExecutor(max_workers=num_instances) as executor:
        future_to_instance = {}
        for instance_id, file_list in enumerate(instance_assignments):
                future = executor.submit(process_main, instance_id, file_list)
                future_to_instance[future] = instance_id

        # 收集结果
        results = []
        for future in as_completed(future_to_instance):
            results.append(future.result())
        return results
ray.init()

# tasks = [remoteJob.remote("a job") for _ in range(4)]  # 假设机器有 4 个 GPU
import time 
from pathlib import Path
s= time.time()
filesRoot='/home/ubuntu/s3bucket/audio'
# filesRoot='/home/ubuntu/whisperASR/Data/Audio_EN'
all_real_files = [str(file) for file in Path(filesRoot).rglob('*') if file.is_file()  ]

# file_list=['/home/ubuntu/whisperASR/Data/Audio_EN/4175.mp3' for _ in range(11)]
process_files_with_instances(all_real_files,4)
# print('client ',ray.get(remoteJob.remote(0,file_list)))
print('duration',time.time()-s)